
model choice如果你不知道选择什么样的模型，那就选择 或者  作为base model。
2.input layer假如你的输入是一张图片，将图片数值归一化到[-1, 1]；
  假如你的输入是一个随机噪声的向量，最好是从N(0, 1)的正太分布里面采样，不要从U(0,1)的均匀分布里采样。
  3.output layer使用输出通道为3的卷积作为最后一层，可以采用1x1或者3x3的filters，有的论文也使用9x9的filters。（注：ganhacks 推荐使用tanh）
  4.transposed convolution layer在做decode的时候，尽量使用upsample+conv2d组合代替transposed_conv2d，可以减少checkerboard的产生
  [5]；在做超分辨率等任务上，可以采用pixelshuffle[6]。在tensorflow里，可以用tf.depth_to_sapce 来实现pixelshuffle 操作。
  5.convolution layer由于笔者经常做图像修复方向相关的工作，推荐使用gated-conv2d[7]
  6.normalization虽然在resnet里的标配是BN，在分类任务上表现很好，但是图像生成方面，推荐使用其他normlization方法，
    例如parameterized 方法有instance normalization[8]、layer normalization[9]等，non-parameterized 方法推荐使用pixel normalization[10]。
    假如你有选择困难症，那就选择大杂烩的normalization 方法——switchable normalization[11]。
  7.discriminator想要生成更高清的图像，推荐multi-stage discriminator[10]。简单的做法就是对于输入图片，把它下采样（maxpooling）到不同scale的大小
    ，输入三个不同参数但结构相同的discriminator。
  8.minibatch discriminator由于判别器是单独处理每张图片
    ，没有一个机制能告诉discriminator每张图片之间要尽可能的不相似，这样就会导致判别器会将所有图片都push到一个看起来真实的点，
    缺乏多样性。minibatch discriminator[22] 就是这样这个机制，显式的告诉discriminator每张图片应该要不相似。在tensorflow中，
    一种实现minibatch discriminator方式如下：上面是通过一个可学习的网络来显示度量每个样本之间的相似度，
    PGGAN里提出了一个更廉价的不需要学习的版本，即通过统计每个样本特征每个像素点的标准差，然后取他们的平均，
    把这个平均值复制到与当前feature map一样空间大小单通道，作为一个额外的feature maps 拼接到原来的feature maps里，
    一个简单的tensorflow 实现如下：
  9.GAN loss除了第二节提到的原始GANs中提出的两种loss，还可以选择wgan loss[12]、hinge loss、lsgan loss[13]等。
    wgan loss 使用Wasserstein 距离（推土机距离）来度量两个分布之间的差异，lsgan 采用类似最小二乘法的思路设计损失函数，
    最后演变成用皮尔森卡方散度代替了原始GAN中的  散度，hinge loss 是迁移了SVM里面的思想，在SAGAN[14]和BigGAN[15]等都是采用该损失函数。
    ps: 我自己经常使用没有relu的hinge loss 版本。
  10. other lossperceptual loss[17]style loss[18]total variation loss[17]
    
  l1 reconstruction loss通常情况下，GAN loss 配合上面几种loss，效果会更好。 
  11. gradient penaltyGradient penalty 首次在wgan-gp 里面提出来的，记为1-gp，目的是为了让discriminator满足1-lipchitchz 
    连续，后续Mescheder, Lars M. et al[19]又提出了只针对正样本或者负样本进行梯度惩罚，记为0-gp-sample。Thanh-Tung, Hoang et al[20]提出了0-gp，
    具有更好的训练稳定性。三者的对比如下：
  12. Spectral normalization[21]谱归一化是另外一个让判别器满足1-lipchitchz 连续的利器，
    建议在判别器和生成器里同时使用。ps:在个人实践中，它比梯度惩罚更有效。
  13. one-size label smoothing[22]平滑正样本的label
    ，例如label 1变成0.9-1.1之间的随机数，保持负样本label仍然为0。个人经验表明这个trick能够有效缓解训练不稳定的现象，
    但是不能根本解决问题，假如模型不够好的话，随着训练的进行，后期loss会飞。
  14. add supervised labelsadd labelsconditional batch normalization15. instance noise(decay over time)在原始GAN中，
    我们其实在优化两个分布的JS散度，前面的推理表明在两个分布的支撑集没有交集或者支撑集是低维的流形空间，他们之间的JS散度大概率上是0；
    而加入instance noise 就是强行让两个分布的支撑集之间产生交集，这样JS散度就不会为0。新的JS散度变为： (12)
  16. TTUR[23]在优化G时候，我们默认是假定我们的D的判别能力是比当前的G的生成能力要好的，这样D才能指导G朝更好的方向学习。
    通常的做法是先更新D的参数一次或者多次，然后在更新G的参数，TTUR提出了一个更简单的更新策略，即分别为D和G设置不同的学习率，让D收敛速度更快。
  17. training strategyPGGAN[10]PGGAN是一个渐进式的训练技巧，因为要生成高清（eg, 1024x1024）的图片，
    直接从一个随机噪声生成这么高维度的数据是比较难的；既然没法一蹴而就，那就循序渐进，首先从简单的低纬度的开始生成，例如4x4，然后16x16，
    直至我们所需要的图片大小。在PGGAN里，首次实现了高清图片的生成，并且可以做到以假乱真，可见其威力。此外，
    由于我们大部分的操作都是在比较低的维度上进行的，训练速度也不比其他模型逊色多少。coarse-to-refinecoarse-to-refine 
    可以说是PGGAN的一个特例，它的做法就是先用一个简单的模型，加上一个l1 loss，训练一个模糊的效果，
    然后再把这个模糊的照片送到后面的refine模型里，辅助对抗loss等其他loss，训练一个更加清晰的效果。这个在图片生成里面广泛应用。
  18.Exponential Moving Average[24]EMA主要是对历史的参数进行一个指数平滑，可以有效减少训练的抖动。强烈推荐！！！
    总结训练GAN是一个精（折）细（磨）的活，一不小心你的GAN可能就是一部惊悚大片。笔者结合自己的经验以及看过的一些文献资料
    ，列出了常用的tricks，在此抛砖引玉，由于笔者能力和视野有限，有些不正确之处或者没补全的tricks，还望斧正。最后，祝大家炼丹愉快，不服就GAN。: )
    
    参考文献Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems. 2014.Arjovsky, Martín and Léon Bottou. “Towards Principled Methods for Training Generative Adversarial Networks.” CoRR abs/1701.04862 (2017): n. pag.Radford, Alec et al. “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.” CoRR abs/1511.06434 (2016): n. pag.He, Kaiming et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016): 770-778.https://distill.pub/2016/deconv-checkerboard/ Shi, Wenzhe et al. “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016): 1874-1883.Yu, Jiahui et al. “Free-Form Image Inpainting with Gated Convolution.” CoRRabs/1806.03589 (2018): n. pag.Ulyanov, Dmitry et al. “Instance Normalization: The Missing Ingredient for Fast Stylization.” CoRR abs/1607.08022 (2016): n. pag.Ba, Jimmy et al. “Layer Normalization.” CoRR abs/1607.06450 (2016): n. pag.Karras, Tero et al. “Progressive Growing of GANs for Improved Quality, Stability, and Variation.” CoRR abs/1710.10196 (2018): n. pag.Luo, Ping et al. “Differentiable Learning-to-Normalize via Switchable Normalization.” CoRRabs/1806.10779 (2018): n. pag.Arjovsky, Martín et al. “Wasserstein GAN.” CoRR abs/1701.07875 (2017): n. pag.Mao, Xudong, et al. "Least squares generative adversarial networks." Proceedings of the IEEE International Conference on Computer Vision. 2017.Gulrajani, Ishaan et al. “Improved Training of Wasserstein GANs.” NIPS (2017).Zhang, Han, et al. "Self-attention generative adversarial networks." arXiv preprint arXiv:1805.08318 (2018).Brock, Andrew, Jeff Donahue, and Karen Simonyan. "Large scale gan training for high fidelity natural image synthesis." arXiv preprint arXiv:1809.11096 (2018).Johnson, Justin et al. “Perceptual Losses for Real-Time Style Transfer and Super-Resolution.” ECCV (2016).Liu, Guilin et al. “Image Inpainting for Irregular Holes Using Partial Convolutions.” ECCV(2018).Thanh-Tung, Hoang et al. “Improving Generalization and Stability of Generative Adversarial Networks.” CoRR abs/1902.03984 (2018): n. pag.Mescheder, Lars M. et al. “Which Training Methods for GANs do actually Converge?” ICML(2018).Yoshida, Yuichi and Takeru Miyato. “Spectral Norm Regularization for Improving the Generalizability of Deep Learning.” CoRR abs/1705.10941 (2017): n. pag.Salimans, Tim et al. “Improved Techniques for Training GANs.” NIPS (2016).Heusel, Martin et al. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” NIPS (2017).Yazici, Yasin et al. “The Unusual Effectiveness of Averaging in GAN Training.” CoRRabs/1806.04498 (2018): n. pag.
